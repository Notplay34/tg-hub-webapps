# Архитектура AI-ассистента TG Hub

## Цель

Единый AI-ассистент: человекоподобный, персонализированный, проактивный. Handlers только передают команды и контекст в AI Service; доступ к задачам, людям и напоминаниям — только через AI (на стороне API).

---

## Поток данных

```
Пользователь (Telegram)
    → Handler (только user_id + message)
    → AI Service.generate_response(user_id, message)
    → API /api/chat (контекст: задачи, люди, знания, финансы, история чата)
    → LLM
    → ответ пользователю
```

- **Бот и handlers не знают SQL.** Данные (tasks, people, reminders) подставляются в контекст на стороне API.
- **TaskService, PeopleService, ReminderService** в смысле «данные для ответа» доступны только внутри API при формировании ответа. Бот вызывает только `generate_response(user_id, message)`.

---

## Компоненты

### 1. AI Service (бот)

- **Интерфейс:** `generate_response(user_id, message) -> str`
- **Реализация:** `ApiAiService` — POST в `/api/chat` с `X-User-Id` и телом `{ "message": "..." }`.
- Контекст диалога и состояние задач хранятся в API (таблица `chat_history` и данные по `user_id`). Опционально бот может хранить свой кэш в `DialogContextManager` (для будущей проактивности).

### 2. API /api/chat

- Собирает контекст: задачи, контакты, знания, финансы, лимиты, последние операции.
- Подставляет дату/время, формирует системный промпт.
- Загружает историю чата из репозитория, добавляет новый поворот, вызывает LLM (OpenRouter и др.).
- Сохраняет ответ в `chat_history`.
- Обрабатывает прямые команды («новый диалог», «забудь про X», создание задачи/контакта/расхода и т.д.) через `parse_user_command` / `execute_ai_action`.

### 3. Контекст и память

- **История диалога:** в API, репозиторий `chat_history` (чтение/запись только в API).
- **Бот:** `DialogContextManager` — опциональный кэш по `user_id` (например, last_topic, last_intent) для будущих проактивных подсказок. Сейчас основная память — в API.

### 4. Проактивность

- **В чате:** системный промпт предписывает при встречах/созвонах/звонках «сегодня или скоро» предлагать подготовиться или напомнить проверить.
- **В напоминаниях:** при отправке напоминания по задаче (RemindersService), если в названии есть «встреча», «созвон», «звонок» — к тексту добавляется «Подготовиться?».

---

## Примеры сценариев

| Запрос пользователя | Поведение |
|---------------------|-----------|
| «Что у меня сегодня?» | API собирает задачи/контекст по user_id, LLM формирует один понятный ответ. |
| «Напомни про встречу с Андреем» | Пользователь может создать задачу с напоминанием через команду в чат (API выполняет действие); AI подсказывает формулировки. |
| Напоминание «Встреча в 15:00» | RemindersService шлёт напоминание с подписью «Подготовиться?». |
| «Очисти диалог» | API очищает историю по user_id и возвращает подтверждение. |

---

## Ограничения архитектуры

- **Handlers:** только передача `user_id` и `message` в AI Service, без бизнес-логики и без прямого доступа к БД/сервисам.
- **AI Service (бот):** единый вход — `generate_response(user_id, message)`; не содержит SQL, не вызывает репозитории напрямую.
- **Доступ к задачам/людям/напоминаниям для ответа:** только в API при построении контекста для LLM; бот эти данные не читает.

---

## Понимание по сырому тексту (без жёстких команд)

Чтобы агент понимал естественные фразы («хочу накопить на отпуск 200к», «потратил 500 на обед», «напомни позвонить маме») и не требовал точных формулировок, возможны такие варианты:

| Способ | Суть | Плюсы | Минусы |
|--------|------|--------|--------|
| **1. Regex первым, ИИ — fallback** | Сначала `parse_user_command` (регулярки). Если ничего не распознано — один вызов ИИ: «определи намерение и извлеки поля» → JSON → `execute_ai_action`. | Быстро для явных команд, ИИ только когда нужно | Два пути, нужно держать формат ответа ИИ стабильным |
| **2. Только ИИ-парсер** | Каждое сообщение сначала отправляется в ИИ: «верни команду (task/expense/goal/contact/note) и параметры в JSON». Регулярки не используются. | Один универсальный вход, гибко | Задержка и токены на каждое сообщение, зависимость от качества модели |
| **3. ИИ в основном чате возвращает команду** | Один вызов LLM: ответ = текст + опциональная команда (например, JSON в отдельном блоке). Бэкенд парсит блок и выполняет действие. | Один запрос, естественный диалог | Модель может забыть команду или ошибиться в формате; нужен надёжный парсинг |
| **4. Tool / function calling** | В API провайдера (OpenRouter и др.) задать инструменты: `add_task`, `add_expense`, `add_goal`, `add_contact`, `add_note` с параметрами. Модель сама выбирает инструмент и заполняет поля. | Стандартный подход, модель обучена на вызове инструментов | Нужна поддержка tools в API и доработка клиента |

**Реализовано в YouHub:** способ 1 — сначала регулярки, при отсутствии совпадения вызывается `extract_command_with_ai(message)`. ИИ возвращает `intent` и поля; бэкенд маппит в существующие действия (`create_task`, `add_finance_transaction`, `add_finance_goal`, `create_person`, `create_knowledge`) и вызывает `execute_ai_action`. Так и явные команды работают быстро, и сырой текст обрабатывается без переписывания всего чата.

---

## Расширение

- **Роли (личный, рабочий, клиенты):** можно добавить в профиль пользователя поле `role` или `context_type` и подставлять его в системный промпт в API.
- **Более богатая проактивность:** использовать `DialogContextManager` на боте и/или отдельные воркеры в API, которые по расписанию проверяют «встреча через час» и отправляют подсказки.
- **Другой LLM:** замена в `api/services/ai_client.py`; контракт «сообщения + системный промпт → ответ» сохраняется.

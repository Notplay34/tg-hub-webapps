# Оценка качества ИИ и советы по улучшению YouHub

**Статус (обновлено):** Все рекомендации реализованы.

## Текущее состояние

### Плюсы
- **Архитектура:** Чёткое разделение: regex → AI-extract → chat. Команды «Что сегодня», «Итоги по деньгам», «Мои цели» — без ИИ, быстро и предсказуемо.
- **Контекст:** Задачи, финансы, люди, проекты, лимиты — всё подставляется в промпт.
- **Память:** История (24 сообщения) + сжатие при >200 сообщений.
- **Команды:** parse_user_command покрывает много фраз; extract_command_with_ai — fallback для естественного текста.

### Проблемы
1. **AgentCore не используется в чате** — persona, memory_summary, intent задуманы в ARCHITECTURE_AI.md, но chat строит system_prompt напрямую, без AgentCore.build_system_prompt.
2. **Промпт перегружен** — много правил и предупреждений, модель легко «теряет» суть.
3. **max_tokens=400** — мало для развёрнутых ответов и анализа.
4. **Модель по умолчанию** — gemma-3-4b-it:free (маленькая); качество extract_command и chat ограничено.
5. **extract_command_with_ai** — один вызов ИИ перед каждым «некомандным» сообщением → двойная задержка (extract + chat).
6. **parse_relative_date** для extract_command — «завтра», «понедельник» обрабатываются, но промпт ИИ об этом не напоминает.

---

## Рекомендации по улучшению

### 1. Подключить AgentCore к чату (высокий приоритет)
Сейчас persona и память не влияют на ответы. Нужно:
- Вызывать `agent_core.load_state(uid)`
- Использовать `agent_core.build_system_prompt(base_prompt, state, intent)` вместо ручного system_prompt
- После ответа вызывать `agent_core.update_memory_after_turn()`

**Эффект:** Персонализация, устойчивый тон, память о предпочтениях.

### 2. Упростить и укоротить системный промпт (высокий)
Сократить блок «Правила» и «Команды» — оставить 3–5 ключевых пунктов. Добавить в начало 1–2 предложения про роль и формат.

**Пример оптимизации:**
```
Ты — ассистент YouHub. Отвечай коротко, по-русски, опираясь только на данные ниже.
Формат: 1–2 предложения + конкретные шаги. При превышении лимита по категории — предупреди.
```

### 3. Увеличить max_tokens (средний)
- Основной чат: 400 → 600–800
- extract_command: 250 оставить (достаточно для JSON)

**Эффект:** Ответы не обрезаются, модель успевает дать совет и шаги.

### 4. Улучшить промпт extract_command_with_ai (средний)
- Добавить сегодняшнюю дату в промпт: «Сегодня YYYY-MM-DD. Завтра = …»
- Явно указать маппинг: «понедельник» → дата следующего понедельника
- Добавить intent `done_task` — «выполнил X», «сделал X», «готово X»

### 5. Разделить модели по назначению (средний)
- `model_hint="summary"` → быстрая дешёвая модель (gemma, gpt-3.5)
- `model_hint="chat"` → более умная модель (если в .env задана)
- `model_hint="extract"` → низкая temperature (0.1), стабильный JSON

В ai_client можно добавить маппинг `{"chat": "gpt-4o-mini", "extract": "gpt-3.5-turbo", "summary": "gemma"}` при наличии ключей.

### 6. Кэшировать результат extract_command для «очевидных» фраз (низкий)
Если текст начинается с «добавь», «создай», «потратил», «напомни» — сразу вызывать extract_command, без regex. Или расширить regex, чтобы не вызывать ИИ для тривиальных случаев.

### 7. Проактивность (низкий)
Промпт уже просит предлагать подготовку к встречам. Стоит усилить: «Если в задачах на сегодня есть встреча/звонок — в конце ответа добавь: «Кстати, у тебя встреча в X — подготовиться?»».

### 8. Обработка ошибок ИИ (низкий)
- При падении extract_command — не падать, идти в обычный чат (уже есть try/except)
- При 429 / quota — сообщение «Лимит исчерпан» (есть)
- Добавить retry (1–2 раза) для network errors

---

## Реализовано

| Что | Файл | Изменение |
|-----|------|-----------|
| AgentCore в чате | api/main.py | load_state, build_system_prompt, update_memory_after_turn, save_state |
| max_tokens чата | api/main.py | 400 → 600 |
| Упростить prompt | api/main.py | base_prompt компактный, persona от AgentCore |
| Дата в extract_command | api/main.py | today_iso + подсказка «завтра»/«понедельник» |
| done_task intent | api/main.py | «выполнил X», «сделал X», «готово X» → complete_task |
| model_hint | api/services/ai_client.py | chat/extract/summary + AI_MODEL_CHAT/EXTRACT/SUMMARY |
| Retry при сетевых ошибках | api/services/ai_client.py | 1 повтор с паузой 1 сек |

---

## Метрики для отслеживания

- % сообщений, где extract_command_with_ai вернул команду (vs none)
- Средняя длина ответа (токены)
- Количество «неправильных» действий (create_task с пустым title и т.п.)
- Время ответа (extract + chat для некоманд)
